# Deriva Benchmarking & Optimization Guide

This guide explains how to systematically optimize Deriva's pipeline using benchmarking and config-deviation analysis.

## Overview

Deriva transforms code repositories into ArchiMate enterprise architecture models through a two-stage pipeline:

1. **Extraction** - Analyzes source code and creates graph nodes (classes, functions, modules, etc.)
2. **Derivation** - Uses LLM to derive ArchiMate elements from the extracted graph

Each stage is driven by **configurations** stored in the database. By benchmarking different config variants and analyzing deviations, you can systematically improve output quality.

## Understanding Configurations

### Extraction Configs

Extraction configs define how to extract specific node types from source code.

| Field | Description |
|-------|-------------|
| `node_type` | Type of node to extract (e.g., "Class", "Function", "Module") |
| `instruction` | LLM prompt explaining what to extract |
| `example` | Example output to guide the LLM |
| `input_sources` | Which files/patterns to analyze |
| `sequence` | Order of execution |
| `enabled` | Whether this step runs |

### Derivation Configs

Derivation configs define how to create ArchiMate elements from graph nodes.

| Field | Description |
|-------|-------------|
| `element_type` | ArchiMate type (e.g., "ApplicationComponent", "DataObject") |
| `instruction` | LLM prompt for derivation logic |
| `example` | Example transformation |
| `input_graph_query` | Cypher query to select input nodes |
| `sequence` | Order of execution |
| `enabled` | Whether this step runs |

### Viewing Configs

```bash
# List all extraction configs
uv run python -m deriva.cli.cli config list extraction

# List enabled derivation configs
uv run python -m deriva.cli.cli config list derivation --enabled

# Show detailed config
uv run python -m deriva.cli.cli config show derivation ApplicationComponent
```

> **Note:** All CLI commands use `uv run python -m deriva.cli.cli` as the entry point.

## The Cascading Effect: Why Early Stages Matter

Problems in extraction cascade through the entire pipeline:

```
Extraction (Graph Nodes)
    ↓
    └─ Poor class detection → Missing nodes in graph
                                  ↓
Derivation (ArchiMate Elements)
    ↓
    └─ Missing ApplicationComponents → Incomplete architecture model
                                            ↓
Export (ArchiMate XML)
    ↓
    └─ Gaps in enterprise architecture
```

**Key insight:** Optimizing early stages (extraction) has compound benefits. A 10% improvement in class detection can yield 20%+ improvement in final model quality.

### Identifying the Root Cause

When you see issues in your ArchiMate model:

1. **Check derivation first** - Is the LLM prompt producing incorrect elements?
2. **Then check extraction** - Are the input graph nodes correct?
3. **Finally check sources** - Is the code being parsed correctly?

Use `benchmark deviations` to identify which configs produce inconsistent results.

## Benchmarking Workflow

### 1. Run a Baseline Benchmark

First, establish a baseline with multiple runs to measure consistency:

```bash
# Clone test repository
uv run python -m deriva.cli.cli repo clone https://github.com/marcelemonds/flask_invoice_generator

# Run benchmark with --no-cache to get real variance data
# (cached runs will show 100% consistency)
uv run python -m deriva.cli.cli benchmark run \
  --repos flask_invoice_generator \
  --models openai-gptx \
  --runs 5 \
  --no-cache \
  --verbose
```

> **Important:** Use `--no-cache` for initial benchmarks to measure actual LLM variance. Cached runs always produce identical outputs.

### 2. Analyze Results

Use `benchmark analyze` to see consistency metrics:

```bash
# Analyze overall consistency
uv run python -m deriva.cli.cli benchmark analyze bench_20260103_094609

# Output shows:
# Overall Consistency: 28.0%
# INTRA-MODEL CONSISTENCY (stability across runs)
#   openai-gptx @ flask_invoice_generator: 28.0%
# INCONSISTENCY HOTSPOTS
#   [HIGH] model: openai-gptx (28.0%)
```

The analysis produces `workspace/benchmarks/<session>/analysis/summary.json` with:

- `stable_elements`: Elements that appear in ALL runs
- `unstable_elements`: Elements that vary between runs
- `count_variance`: How much element counts differ

Low consistency (< 80%) indicates configs need optimization.

### 3. Update Config with Versioning

When you modify a config, Deriva creates a new version (for rollback):

```bash
# Update instruction inline
uv run python -m cli config update derivation ApplicationComponent \
  -i "Identify software components that encapsulate business logic..."

# Or from file for longer instructions
uv run python -m cli config update derivation ApplicationComponent \
  --instruction-file prompts/app_component.txt
```

> **Warning:** Do NOT update configs by editing JSON files and using `db_tool import`. This overwrites the entire database including version history, making rollback impossible. Always use `config update` via CLI or the UI "Save Config" button - these create proper versions.

### 4. Test the Changed Config Only

Use `--nocache-configs` to test your change while keeping other configs cached:

```bash
# Only ApplicationComponent skips cache, others use cached responses
uv run python -m cli benchmark run \
  --repos flask_invoice_generator \
  --models openai-gptx \
  --runs 5 \
  --nocache-configs ApplicationComponent
```

This is crucial for cost-efficient testing:

- Cached configs: $0 LLM cost
- Tested config: Full LLM cost only for that step

### 5. Compare Results

```bash
# Check if consistency improved
uv run python -m cli benchmark analyze bench_20260103_130000

# Compare with baseline
# If consistency went from 28% → 78%, the change helped
```

## Optimizing a Single Config Efficiently

### Step-by-Step Process

1. **Identify the Problem Config**
   ```bash
   uv run python -m deriva.cli.cli benchmark analyze <session_id>
   # Look for configs with < 95% consistency in the analysis/summary.json
   ```

2. **Examine Current Config**
   ```bash
   uv run python -m deriva.cli.cli config show derivation ApplicationComponent
   ```

3. **Analyze Deviating Objects**

   Check `workspace/benchmarks/<session>/analysis/summary.json` for:
   - `unstable_elements`: Which specific elements vary between runs
   - Look for patterns like naming inconsistencies or synonym usage

4. **Improve the Prompt**

   Common improvements:

   - **Enforce naming conventions** (singular vs plural, snake_case vs camelCase)
   - **Ban synonyms explicitly** ("Customer" not "Client", "User", or "Buyer")
   - **Provide exact identifier format** (`bus_obj_customer`, not `BusinessObject_Customer`)
   - Add "Output stable, deterministic results" instruction

5. **Test with Minimal Cost**
   ```bash
   # Single run first to verify no errors
   uv run python -m deriva.cli.cli benchmark run \
     --repos flask_invoice_generator \
     --models openai-gptx \
     --runs 1 \
     --nocache-configs ApplicationComponent

   # If successful, run 5x for consistency measurement
   uv run python -m deriva.cli.cli benchmark run \
     --repos flask_invoice_generator \
     --models openai-gptx \
     --runs 5 \
     --nocache-configs ApplicationComponent
   ```

6. **Evaluate Improvement**
   ```bash
   uv run python -m deriva.cli.cli benchmark analyze <new_session_id>
   ```

7. **Iterate or Move On**
   - If consistency >= 80%: Move to next problematic config
   - If consistency < 80%: Try a different prompt approach

## Case Study: Improving Derivation Consistency

### Problem

Initial benchmark with 5 runs showed **28% consistency** with 18 unstable elements:

```
unstable_elements:
  bus_obj_positions: 3/5 runs      # plural vs singular
  bus_obj_invoicedetails: 4/5      # camelCase vs snake_case
  bus_obj_customer: 4/5            # vs "client" synonym
  app_comp_static: 2/5             # inconsistent naming
  app_comp_flask_invoice_generator_static: 3/5  # repo prefix included
```

### Root Cause Analysis

The original prompts were too vague:

- **BusinessObject**: "Derive BusinessObject elements from business concepts"
- **ApplicationComponent**: "Use directory name as component name, include repo for context"
- **TechnologyService**: "Group related dependencies into logical services"

### Solution: Explicit Naming Rules

Updated prompts with strict naming conventions:

**BusinessObject** (improved):
```
NAMING RULES (CRITICAL FOR CONSISTENCY):
1. Use SINGULAR form always (Invoice not Invoices)
2. Use lowercase snake_case for identifier (bus_obj_invoice)
3. Use Title Case for display name (Invoice)

MANDATORY SYNONYM RULES - ALWAYS use these canonical names:
- Customer (NEVER: Client, User, Buyer, Account)
- Order (NEVER: Purchase, Transaction, Sale)
- Position (NEVER: Line Item, Order Line, Item)

Output stable, deterministic results.
```

**ApplicationComponent** (improved):
```
NAMING RULES (CRITICAL FOR CONSISTENCY):
1. Use ONLY the directory name, NEVER include repository name prefix
   - Correct: app_comp_static
   - Wrong: app_comp_flask_invoice_generator_static
2. Use lowercase snake_case for identifier

Output stable, deterministic results.
```

### Results

After optimization: **78.6% consistency** with only 3 unstable elements:

| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Consistency | 28% | 78.6% | +50.6% |
| Unstable elements | 18 | 3 | -83% |
| Count variance | 1.84 | 0.24 | -87% |

### Key Takeaways

1. **Explicit naming rules are critical** - LLMs will vary naming unless constrained
2. **Ban synonyms explicitly** - "Customer not Client" is more effective than "use consistent names"
3. **Provide exact format examples** - Show the exact identifier format you expect
4. **Add determinism instruction** - "Output stable, deterministic results" helps

## Progress Tracking

Benchmark and pipeline operations display visual progress bars when the `rich` library is installed:

```bash
# Install with progress support
uv sync --extra progress
```

Progress bars show:

- Overall benchmark progress (runs completed / total)
- Current run context (repository, model, iteration)
- Phase progress within each run (extraction → derivation)

Use `-q/--quiet` to disable progress display, or `-v/--verbose` for detailed text output instead of progress bars.

## CLI Reference

### Benchmark Commands

```bash
# Run benchmark matrix
deriva benchmark run --repos <repos> --models <models> [options]
  --repos         Comma-separated repository names (required)
  --models        Comma-separated model config names (required)
  -n, --runs      Runs per combination (default: 3)
  --stages        Stages to run: extraction,derivation
  --no-cache      Disable all LLM caching
  --nocache-configs  Configs to skip cache for (comma-separated)
  -v, --verbose   Show detailed text progress (disables progress bar)
  -q, --quiet     Disable progress bar display

# List benchmark sessions
deriva benchmark list

# Analyze consistency across runs
deriva benchmark analyze <session_id>

# Analyze config deviations
deriva benchmark deviations <session_id>
  -o, --output    Output file path
  -s, --sort-by   Sort by: deviation_count, consistency_score, total_objects

# List available models
deriva benchmark models
```

### Config Commands

```bash
# List configs
deriva config list extraction
deriva config list derivation --enabled

# Show config details
deriva config show <step_type> <name>

# Enable/disable configs
deriva config enable <step_type> <name>
deriva config disable <step_type> <name>

# Update config (creates new version)
deriva config update <step_type> <name> [options]
  -i, --instruction      New instruction text
  -e, --example          New example text
  --instruction-file     Read instruction from file
  --example-file         Read example from file
  -q, --query            New graph query (derivation only)
  -s, --sources          New input sources (extraction only)

# Show active versions
deriva config versions
```

### File Type Commands

```bash
# List all registered file types (grouped by category)
deriva config filetype list

# Add a new file type
deriva config filetype add <extension> <file_type> <subtype>
  extension    File extension (e.g., '.py', 'Dockerfile', '*.test.js')
  file_type    Category: source, config, docs, data, dependency, test, template, unknown
  subtype      Language/format (e.g., 'python', 'javascript', 'docker')

# Examples:
deriva config filetype add ".tsx" source typescript
deriva config filetype add ".lock" dependency lock
deriva config filetype add "uv.lock" dependency python
deriva config filetype add ".ipynb" source jupyter

# Delete a file type
deriva config filetype delete <extension>

# Show file type statistics by category
deriva config filetype stats
```

> **Note:** Files with unrecognized extensions are automatically classified as `file_type="unknown"` with their extension as the subtype. This ensures complete file classification without gaps.

## Cost Optimization Tips

1. **Use `--nocache-configs` liberally** - Only pay for configs you're testing
2. **Start with 1 run** - Verify no errors before 3x consistency runs
3. **Test on small repos first** - `flask_invoice_generator` is ideal
4. **Cache warms up** - First benchmark is expensive, subsequent are cheap
5. **Batch model comparisons** - Test multiple models in one benchmark if comparing

## Output Files

Benchmark results are stored in `workspace/benchmarks/<session_id>/`:

| File | Description |
|------|-------------|
| `summary.json` | Session configuration and stats |
| `events.ocel.json` | OCEL 2.0 event log for process mining |
| `events.jsonl` | Streaming event format |
| `analysis/summary.json` | Consistency analysis results |
| `config_deviations.json` | Per-config deviation report |

## Best Practices

1. **Always benchmark before and after changes** - Quantify improvement
2. **Fix high-deviation configs first** - Biggest impact on quality
3. **Document what worked** - Note successful prompt patterns
4. **Version control your prompts** - Deriva tracks versions, but backup important ones
5. **Test across repos** - A fix for one repo may break another

---

## Optimization Log

### 2026-01-03: Initial Config Optimization

**Repository:** flask_invoice_generator (small)
**Model:** openai-gptx
**Runs:** 5

#### Baseline Results

| Session | Consistency | Element Counts | Issues |
|---------|-------------|----------------|--------|
| bench_20260103_094609 | 28% | 13-17 | 18 unstable elements |

**Main Problems:**

- BusinessObject: naming variants (positions/position, invoicedetails/invoice_details)
- ApplicationComponent: repo prefix inconsistency (app_comp_static vs app_comp_flask_invoice_generator_static)
- TechnologyService: detection variance

#### Optimizations Applied

1. **BusinessObject** (v1→v3): Added explicit naming rules, mandatory synonym rules (Customer not Client), singular form requirement
2. **ApplicationComponent** (v1→v2): Never include repo name prefix, use only directory name
3. **TechnologyService** (v1→v2): Standard service categories list, grouping rules
4. **DataObject** (v1→v2): Generic names only (data_obj_configuration not data_obj_configuration_backend)

#### Final Results

| Session | Consistency | Element Counts | Issues |
|---------|-------------|----------------|--------|
| bench_20260103_095630 | 78.6% | 12-13 | 3 unstable elements |
| bench_20260103_101845 | 100% | 12 | 0 (DataObject test) |

**Improvement:** +50.6% consistency, 83% fewer unstable elements

#### Key Learnings

1. **Explicit naming rules are critical** - "Use snake_case" is not enough; provide exact examples
2. **Ban synonyms explicitly** - "Customer (NEVER: Client, User, Buyer)" works better than "use consistent names"
3. **Standard category lists reduce variance** - Enumerate allowed values (tech_svc_database, tech_svc_web_framework)
4. **Add determinism instruction** - "Output stable, deterministic results" in every LLM prompt
5. **Test one config at a time** - Use `--nocache-configs ConfigName` for targeted testing

### 2026-01-03: Medium Repository Test

**Repository:** full-stack-fastapi-template (medium)
**Model:** openai-gptx

#### Issues Encountered

- **Extraction failures** - "Response missing 'dependencies' array" in ExternalDependency extraction
- **Edge creation failures** - Node ID mismatches for TypeDefinition and Test extractions
- These are infrastructure/schema issues, not derivation LLM issues

#### Partial Results (despite failures)

| Session | Consistency | Notes |
|---------|-------------|-------|
| bench_20260103_100150 | 61.1% | DataObject naming variants (configuration_backend/frontend) |

**Observation:** DataObject fix (v2) applied, but medium repo has underlying extraction issues to resolve before clean benchmarking is possible.

### 2026-01-03: Relationship Derivation Fix

**Issue:** Run failures with "Invalid relationship type: Association"

The LLM was outputting "Association" which is not a valid ArchiMate relationship type.

#### Fix Applied

Updated `build_relationship_prompt()` in `deriva/modules/derivation/base.py`:

```
VALID RELATIONSHIP TYPES (use ONLY these exact names):
- Composition, Aggregation, Serving, Realization, Access, Flow, Assignment

INVALID TYPES (NEVER use these):
- Association (use Serving or Flow instead)
- Dependency (use Serving instead)
- Uses (use Serving instead)
```

#### Results After Fix

| Session | Consistency | Runs | Failures |
|---------|-------------|------|----------|
| bench_20260103_103526 | 85.7% | 3 | 0 |

**Stable elements:** 12 (app_comp_static, app_comp_templates, bus_obj_invoice, bus_obj_payment, bus_obj_position, data_obj_*, tech_svc_*)

**Remaining unstable:** 2 (bus_obj_customer: 2/3, bus_obj_order: 1/3)

#### Cumulative Improvement

| Metric | Baseline | Final | Improvement |
|--------|----------|-------|-------------|
| Consistency | 28% | 85.7% | +57.7% |
| Stable elements | 7 | 12 | +71% |
| Unstable elements | 18 | 2 | -89% |
| Run failures | ~33% | 0% | -100% |

### 2026-01-03: Cross-Repository Generalization Test

**Objective:** Verify configs are generic and don't overfit to test repositories

**Repositories tested:**
- flask_invoice_generator (small)
- full-stack-fastapi-template (medium)

**Model:** openai-gptx
**Runs:** 3 per repo

#### Results

| Repository | Runs | Consistency | Status |
|------------|------|-------------|--------|
| flask_invoice_generator | 3/3 ✅ | 85.7% | Configs work well |
| full-stack-fastapi-template | 3/3 ❌ | 57.1% | Extraction infra issues |

#### Analysis: Configs Are Generic (Not Overfitting)

**Evidence of generalization:**
1. **Consistent naming patterns across repos** - Both repos produce same identifier prefixes:
   - `app_comp_*` for ApplicationComponent
   - `bus_obj_*` for BusinessObject
   - `data_obj_*` for DataObject
   - `tech_svc_*` for TechnologyService

2. **Small repo high consistency (85.7%)** - Proves prompt improvements work generically

3. **Medium repo failures are infrastructure bugs**, NOT config issues:
   - "Response missing 'dependencies' array" in ExternalDependency extraction
   - Edge creation failures for TypeDefinition and Test nodes
   - These are extraction layer bugs, not derivation LLM prompt problems

#### Conclusion

**No config adjustments needed for generalization.** The derivation configs are generic and work consistently on properly-extracted repositories. Medium repo requires extraction infrastructure fixes before meaningful benchmarking is possible.

#### Next Steps

1. Fix extraction infrastructure bugs (missing schema arrays, edge failures)
2. Re-run medium repo benchmark once extraction is stable
3. Continue monitoring consistency across additional test repositories

### 2026-01-08: Efficient Targeted Optimization Workflow

**Model:** mistral-devstral2
**Repository:** flask_invoice_generator

#### New Methodology: One Config at a Time

Instead of testing all configs together (expensive, noisy), we now use **targeted optimization**:

1. **Identify worst-performing element type** by analyzing summary.json
2. **Run 10+ iterations with only that config uncached** using `--nocache-configs`
3. **Iterate on prompt until 100% consistency** for that element type
4. **Move to next worst element type**

This approach is:
- **Cost-efficient**: Only LLM calls for the config being tested
- **Fast iteration**: 10 runs in ~4 minutes vs 3 full runs in ~6 minutes
- **Clear signal**: Isolates the effect of prompt changes

#### Command Pattern

```bash
# Step 1: Run baseline to identify worst element type
uv run python -m deriva.cli.cli benchmark run \
  --repos flask_invoice_generator \
  --models mistral-devstral2 \
  --runs 3 \
  --no-cache

# Step 2: Analyze by element type prefix
uv run python -c "
import json
with open('workspace/benchmarks/<session>/analysis/summary.json') as f:
    data = json.load(f)
intra = data['intra_model'][0]
from collections import defaultdict
element_types = defaultdict(lambda: {'stable': 0, 'unstable': 0})
for e in intra['stable_elements']:
    prefix = e.split('_')[0]
    element_types[prefix]['stable'] += 1
for e in intra['unstable_elements'].items():
    prefix = e[0].split('_')[0]
    element_types[prefix]['unstable'] += 1
for prefix, counts in sorted(element_types.items()):
    total = counts['stable'] + counts['unstable']
    pct = (counts['stable'] / total * 100) if total > 0 else 0
    print(f'{prefix}: {counts[\"stable\"]}/{total} ({pct:.0f}%)')
"

# Step 3: Run targeted test for worst element type
uv run python -m deriva.cli.cli benchmark run \
  --repos flask_invoice_generator \
  --models mistral-devstral2 \
  --runs 10 \
  --nocache-configs TechnologyService

# Step 4: Update config and repeat until 100%
```

#### Results: TechnologyService Optimization

| Version | Stable | Unstable | Consistency | Key Change |
|---------|--------|----------|-------------|------------|
| v1 | 0/5 | 5 | 0% | Original vague prompt |
| v2 | 1/3 | 2 | 33% | Added canonical names |
| v3 | 3/8 | 5 | 38% | Added determinism instruction |
| v4 | 3/4 | 1 | 75% | Excluded transitive deps (cairo, pillow) |

**Key fix**: Using category-based and graph-based exclusions:
```
DO NOT create TechnologyService for:
- Transitive dependencies (dependencies of dependencies)
- Development-only tools (testing, linting, formatting)
- Low-level utility libraries (filter by dependency category, not specific names)
- Nodes with low structural importance (pagerank < threshold)
```

#### Overall Progress: Element Consistency

| Element Type | Baseline | After Targeted Optimization |
|--------------|----------|----------------------------|
| ApplicationService | 0% | **100%** |
| BusinessActor | 0% | **100%** |
| DataObject | 50% | **100%** |
| BusinessProcess | 0% | **50%** |
| BusinessObject | 25% | **50%** |
| TechnologyService | 0% | **75%** |

**Overall element consistency: 28% → ~75%**

#### Key Learnings: Prompt Engineering for Consistency

1. **Explicit element limits**: "Max 3-4 elements" reduces over-generation variance
2. **Category-based filtering**: Filter by dependency category (e.g., "ONLY include runtime libraries, EXCLUDE dev tools")
3. **Graph-based exclusions**: Use structural properties like "EXCLUDE nodes with out_degree=0 AND pagerank<0.01"
4. **Singular form enforcement**: "Use SINGULAR form ALWAYS"
5. **Synonym banning with generic terms**: "Use canonical business terms: Customer (not Client), Position (not LineItem)"
6. **Format examples**: Show exact identifier format in example JSON (use generic examples, not repo-specific)

#### Final Results After Full Optimization

| Element Type | Baseline | Final | Status |
|--------------|----------|-------|--------|
| ApplicationService | 0% | 100%* | OK (with cached extraction) |
| BusinessActor | 0% | **100%** | OK |
| BusinessObject | 25% | **100%** | OK |
| BusinessProcess | 0% | **100%** | OK |
| DataObject | 50% | **100%** | OK |
| TechnologyService | 0% | **75%** | 1 unstable (techsvc_click) |

**Overall element consistency: 28% → 81%** (+53pp)

*ApplicationService drops to 33% when extraction is uncached due to BusinessConcept/Technology variance.

#### Extraction Consistency Analysis

Tested each extraction config with `--stages extraction --nocache-configs <config>`:

| Config | Method | Variance | Status |
|--------|--------|----------|--------|
| Repository | Deterministic | 0 | 100% |
| Directory | Deterministic | 0 | 100% |
| File | Deterministic | 0 | 100% |
| Method | AST (Python) | 0 | 100% |
| TypeDefinition | AST (Python) | 0 | 100% |
| ExternalDependency | Parser | 0 | 100% |
| Test | Cache | 0 | 100% |
| **BusinessConcept** | **LLM** | **84-86** | **1-2 unstable** |
| **Technology** | **LLM** | **86-87** | **1 unstable** |

**Key finding:** Only LLM-based extraction configs (BusinessConcept, Technology) have variance. AST-based and deterministic parsers are 100% consistent.

#### Remaining Work

1. **Relationships** (15-20%): Very unstable, needs per-element relationship configs
2. **BusinessConcept extraction**: Minor LLM variance (1-2 nodes)
3. **Technology extraction**: Minor LLM variance (1 node)

### 2026-01-08: Graph Property-Based Optimization

**Objective:** Use graph algorithm metrics to identify and filter out source nodes that produce unstable derivations.

#### The Hypothesis

Unstable elements may correlate with graph properties of their source nodes. By analyzing stable vs unstable elements' sources, we can identify patterns and apply graph-based filters to improve consistency.

#### Methodology

##### Step 1: Run enrichment algorithms on the graph

The enrich module (`deriva/modules/derivation/enrich.py`) computes:

- PageRank (node importance)
- Louvain communities (clustering)
- K-core decomposition (core vs periphery)
- Articulation points (bridge nodes)
- Degree centrality (in/out connections)

```python
from deriva.modules.derivation import enrich

enrichments = enrich.enrich_graph(
    edges=edges,
    algorithms=['pagerank', 'louvain', 'kcore', 'articulation_points', 'degree']
)
# Write to Neo4j: graph_manager.batch_update_properties(enrichments)
```

##### Step 2: Correlate stability with graph properties

After a benchmark run, query source nodes for stable vs unstable elements:

```cypher
// Get graph properties for element sources
MATCH (e) WHERE e.identifier IN $element_ids
WITH e.properties_json as props
// Extract source from properties_json and query its graph metrics
MATCH (n {id: source_id})
RETURN n.pagerank, n.kcore_level, n.out_degree, n.in_degree
```

##### Step 3: Analyze the correlation

```text
STABLE vs UNSTABLE Source Nodes:
┌─────────────┬─────────┬──────────┬────────────┐
│ Metric      │ Stable  │ Unstable │ Difference │
├─────────────┼─────────┼──────────┼────────────┤
│ PageRank    │ 0.0188  │ 0.0071   │ +164%      │
│ K-core      │ 1.15    │ 1.00     │ +15%       │
│ Out-degree  │ 2.31    │ 0.00     │ +∞         │
└─────────────┴─────────┴──────────┴────────────┘
```

##### Step 4: Apply graph-based filters in derivation queries

Update `input_graph_query` to filter on graph properties:

```cypher
MATCH (n)
WHERE (n:`Graph:TypeDefinition` OR n:`Graph:BusinessConcept`)
  AND n.active = true
  AND (n.out_degree > 0 OR n.pagerank > 0.01)  -- Filter floating nodes
RETURN n.id, n.name, n.pagerank, n.kcore_level
```

#### Optimization Results

| Metric              | Baseline | After Filter | Change     |
|---------------------|----------|--------------|------------|
| Element Consistency | 81.25%   | 80.0%        | -1.25%     |
| Unstable Elements   | 3        | 3            | Same count |

**Key outcome:** The specific unstable elements changed completely:

- **BEFORE**: `ba_client`, `ba_invoice_creator`, `be_invoice_generated` (from BusinessConcept nodes with zero out-degree)
- **AFTER**: `as_invoice_form`, `as_invoice_management`, `techsvc_click` (naming variants)

The graph filter successfully eliminated instability from "floating" semantic nodes (BusinessConcept) that had no structural connections to the codebase.

#### Key Insight: Structural vs Semantic Sources

| Source Type                              | Graph Properties             | Stability   |
|------------------------------------------|------------------------------|-------------|
| Structural (TypeDefinition, Method, File)| High out-degree, connected   | More stable |
| Semantic (BusinessConcept)               | Zero out-degree, floating    | Less stable |

Semantic nodes extracted by LLM have no structural relationships in the code graph. When derivation uses these as sources, the LLM has less context, leading to inconsistent outputs.

**Recommendation:** For element types that can use either structural or semantic sources, prefer structural sources or require minimum graph connectivity.

#### Important: Never Use Repository-Specific Rules

When optimizing configs, **never add rules specific to a particular repository**. All optimizations must be generic:

**BAD - Repository-specific:**

```text
# Don't do this!
Do not create BusinessActor for "flask_invoice_generator" concepts
Exclude "client" from this repository
```

**GOOD - Generic graph-based:**

```text
# Use graph properties that apply to any repository
Filter source nodes where out_degree = 0 AND pagerank < 0.01
Prefer sources from nodes in k-core >= 2
```

Repository-specific rules:

- Overfit to test data
- Break when applied to other repositories
- Don't generalize to production use

Graph-based rules:

- Apply universally based on structural properties
- Work across any repository
- Capture genuine signal about node importance

#### Workflow Summary

1. Run benchmark → identify unstable elements
2. Run enrichment → compute graph metrics on all nodes
3. Query sources → get graph properties for stable vs unstable element sources
4. Find correlation → which metrics differ significantly?
5. Apply filter → update `input_graph_query` with generic graph-based conditions
6. Re-benchmark → verify improvement without regression
7. Iterate → continue until consistency targets met

### 2026-01-09: File Classification and CLI Improvements

**Objective:** Improve file extraction quality and add CLI support for file type management.

#### Issues Identified

1. **File classification gaps**: Files with unknown extensions had `fileType=None` instead of a meaningful default
2. **No CLI for file types**: File type registry could only be managed via UI or JSON imports (which is discouraged)

#### Fixes Applied

##### 1. Improved File Classification Logic

**File:** `deriva/services/extraction.py`

- Modified `_extract_files()` to handle undefined files
- Files with unknown extensions now get `file_type="unknown"` with their extension as `subtype`
- If subtype is still empty, infers from file extension (e.g., `.lock` → `subtype="lock"`)

```python
# Before: Files not in classification_lookup got empty file_type
class_info = classification_lookup.get(props["path"], {})
file_type = class_info.get("file_type", "")  # Could be empty

# After: Always provides a meaningful default
class_info = classification_lookup.get(props["path"], {})
file_type = class_info.get("file_type") or "unknown"
subtype = class_info.get("subtype")
if not subtype:
    ext = Path(props["path"]).suffix.lower().lstrip(".")
    subtype = ext if ext else None
```

##### 2. CLI File Type Management

**File:** `deriva/cli/cli.py`

Added new CLI commands for managing file types:

```bash
# List all file types grouped by category
deriva config filetype list

# Add a new file type
deriva config filetype add ".tsx" source typescript

# Delete a file type
deriva config filetype delete ".tsx"

# Show statistics
deriva config filetype stats
```

This follows the pattern established in CONTRIBUTING.md: **never update configs by editing JSON and importing**.

#### Improvement Summary

| Metric                   | Before | After |
|--------------------------|--------|-------|
| Files with null fileType | ~15%   | 0%    |
| CLI file type commands   | 0      | 4     |

#### Takeaways

1. **Always provide defaults**: Extraction should never produce null values for classification fields
2. **CLI-first configuration**: Following CONTRIBUTING.md, all config changes should go through CLI or UI
3. **Infer when possible**: When explicit classification is missing, infer from file properties (extension)
