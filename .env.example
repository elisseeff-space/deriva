# ==============================================================================
# Deriva Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your values
# .env is gitignored for security

# ==============================================================================
# NEO4J CONFIGURATION
# ==============================================================================
# Neo4j connection (shared by graph_manager and archimate_manager)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=
NEO4J_PASSWORD=
NEO4J_DATABASE=neo4j

# Neo4j connection pool settings
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=60
NEO4J_ENCRYPTED=false
NEO4J_TRUST=TRUST_ALL_CERTIFICATES

# Neo4j logging
NEO4J_LOG_LEVEL=INFO
NEO4J_LOG_QUERIES=false

# Manager namespaces (Neo4j label prefixes)
NEO4J_NAMESPACE_GRAPH=Graph
NEO4J_NAMESPACE_ARCHIMATE=Model

# ==============================================================================
# LLM CONFIGURATION
# ==============================================================================
# Default model: Reference a benchmark model config name (e.g., "azure-gpt4mini")
# This uses the model defined in LLM_* settings below
LLM_DEFAULT_MODEL=azure-gpt4mini

# Temperature for LLM queries (0.0-2.0, lower = more deterministic)
# Recommended: 0.3-0.5 for consistent extraction, 0.7 for general use
LLM_TEMPERATURE=0.6

# LLM cache settings
LLM_CACHE_DIR=workspace/cache
LLM_CACHE_TTL=0
LLM_MAX_RETRIES=3
LLM_TIMEOUT=60

# Nocache setting
LLM_NOCACHE='false'

# ==============================================================================
# DATABASE CONFIGURATION (DuckDB)
# ==============================================================================
# Database is at: deriva/adapters/database/sql.db (hardcoded)
# DATABASE_PATH env var is not used by the current code

# ==============================================================================
# REPOSITORY MANAGER CONFIGURATION
# ==============================================================================
# Workspace directory for cloned repositories and runtime data
REPOSITORY_WORKSPACE_DIR=workspace/repositories

# Repository defaults
REPOSITORY_DEFAULT_BRANCH=main
REPOSITORY_DEFAULT_CLONE_DEPTH=

# ==============================================================================
# GRAPH MANAGER CONFIGURATION
# ==============================================================================
# Graph manager namespace (should match NEO4J_NAMESPACE_GRAPH)
GRAPH_NAMESPACE=Graph

# Graph logging
GRAPH_LOG_LEVEL=INFO

# ==============================================================================
# ARCHIMATE MANAGER CONFIGURATION
# ==============================================================================
# ArchiMate manager namespace (should match NEO4J_NAMESPACE_ARCHIMATE)
ARCHIMATE_NAMESPACE=Model

# Graph logging
GRAPH_LOG_LEVEL=INFO

# ArchiMate model settings
ARCHIMATE_VERSION=3.1
ARCHIMATE_IDENTIFIER_PREFIX=id-

# ArchiMate namespaces for XML export
ARCHIMATE_XML_NAMESPACE=http://www.opengroup.org/xsd/archimate/3.0/
ARCHIMATE_XML_DC_NAMESPACE=http://purl.org/dc/elements/1.1/
ARCHIMATE_XML_XSI_NAMESPACE=http://www.w3.org/2001/XMLSchema-instance

# Supported element types (comma-separated)
ARCHIMATE_ELEMENT_TYPES=ApplicationComponent,ApplicationInterface,ApplicationService,DataObject

# Supported relationship types (comma-separated)
ARCHIMATE_RELATIONSHIP_TYPES=Composition,Aggregation,Assignment,Realization,Serving,Access,Flow

# Validation settings
ARCHIMATE_VALIDATION_STRICT_MODE=false
ARCHIMATE_VALIDATION_ALLOW_CUSTOM_PROPERTIES=true

# Export settings
ARCHIMATE_EXPORT_PRETTY_PRINT=true
ARCHIMATE_EXPORT_ENCODING=UTF-8
ARCHIMATE_EXPORT_XML_DECLARATION=true
ARCHIMATE_EXPORT_VALIDATE_ON_EXPORT=true
ARCHIMATE_EXPORT_INCLUDE_METADATA=true

# ==============================================================================
# APPLICATION SETTINGS
# ==============================================================================
# Application environment: development, production
APP_ENV=development

# Logging
APP_LOG_LEVEL=INFO
APP_LOG_DIR=logs

# Output directory for generated ArchiMate files
OUTPUT_DIR=output

# ==============================================================================
# BENCHMARKING - Model Configurations
# ==============================================================================
# Naming: LLM_{NAME}_* → CLI name is lowercase with hyphens (AZURE_GPT4MINI → azure-gpt4mini)
# Timing: Measured on simple structured JSON extraction task
#
# Model Performance Summary:
#   azure-gpt4mini:    ~1.2s  (cloud, fastest, reliable JSON)
#   anthropic-haiku:   ~1.5s  (cloud, fast, reliable JSON)
#   anthropic-sonnet4: ~2.5s  (cloud, balanced performance/cost)
#   ollama-devstral:   ~4.3s  (local, devstral-small-2, good for code)
#   ollama-gptoss:     ~8.4s  (local, gpt-oss:20b, larger model)
#   ollama-gemma3:     ~9.4s  (local, gemma3:4b, FAILS structured JSON)
#   ollama-qwen3vl:    ~16.6s (local, qwen3-vl:8b, vision-language model)
#   ollama-olmo3:      ~40.3s (local, olmo-3:7b, slowest)
# ==============================================================================

# Azure GPT-4o-mini (cloud) → use as: azure-gpt4mini
# Fast cloud model (~1.2s), reliable structured JSON output
LLM_AZURE_GPT4MINI_PROVIDER=azure
LLM_AZURE_GPT4MINI_MODEL=gpt-4o-mini
LLM_AZURE_GPT4MINI_URL=https://your-resource.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview
LLM_AZURE_GPT4MINI_KEY=your-azure-api-key

# Ollama gemma3:4b (local) → use as: ollama-gemma3
# WARNING: Often fails to produce valid structured JSON output
LLM_OLLAMA_GEMMA3_PROVIDER=ollama
LLM_OLLAMA_GEMMA3_MODEL=gemma3:4b
LLM_OLLAMA_GEMMA3_URL=http://localhost:11434/api/chat

# Ollama olmo-3:7b (local) → use as: ollama-olmo3
# Slowest local model (~40s), but reliable JSON output
LLM_OLLAMA_OLMO3_PROVIDER=ollama
LLM_OLLAMA_OLMO3_MODEL=olmo-3:7b
LLM_OLLAMA_OLMO3_URL=http://localhost:11434/api/chat

# Ollama devstral-small-2 (local) → use as: ollama-devstral
# Fast local model (~4.3s), Mistral's code-focused model
LLM_OLLAMA_DEVSTRAL_PROVIDER=ollama
LLM_OLLAMA_DEVSTRAL_MODEL=devstral-small-2
LLM_OLLAMA_DEVSTRAL_URL=http://localhost:11434/api/chat

# Ollama gpt-oss:20b (local) → use as: ollama-gptoss
# Medium speed (~8.4s), larger 20B parameter model
LLM_OLLAMA_GPTOSS_PROVIDER=ollama
LLM_OLLAMA_GPTOSS_MODEL=gpt-oss:20b
LLM_OLLAMA_GPTOSS_URL=http://localhost:11434/api/chat

# Ollama qwen3-vl:8b (local) → use as: ollama-qwen3vl
# Vision-language model (~16.6s), can process images
LLM_OLLAMA_QWEN3VL_PROVIDER=ollama
LLM_OLLAMA_QWEN3VL_MODEL=qwen3-vl:8b
LLM_OLLAMA_QWEN3VL_URL=http://localhost:11434/api/chat

# ==============================================================================
# LM STUDIO MODELS (local, OpenAI-compatible)
# ==============================================================================
# LM Studio provides an OpenAI-compatible API at localhost:1234
# No API key required - runs locally
# Download models via LM Studio app: https://lmstudio.ai

# LM Studio local model → use as: lmstudio-local
# Uses whatever model is loaded in LM Studio
LLM_LMSTUDIO_LOCAL_PROVIDER=lmstudio
LLM_LMSTUDIO_LOCAL_MODEL=local-model
LLM_LMSTUDIO_LOCAL_URL=http://localhost:1234/v1/chat/completions

# ==============================================================================
# CLAUDE CODE MODELS (via Agent SDK CLI)
# ==============================================================================
# Uses your authenticated Claude Code session - no API key needed!
# Requires: Claude Code extension installed and authenticated
#
# Model Pricing (per Million Tokens):
#   haiku:  $1/$5   (fastest, cheapest)
#   sonnet: $3/$15  (balanced)
#   opus:   $5/$25  (most capable)

# Claude Haiku 4.5 (via Claude Code) → use as: claudecode-haiku
# FASTEST & CHEAPEST - recommended for most extraction tasks
LLM_CLAUDECODE_HAIKU_PROVIDER=claudecode
LLM_CLAUDECODE_HAIKU_MODEL=haiku
LLM_CLAUDECODE_HAIKU_URL=cli
LLM_CLAUDECODE_HAIKU_KEY=

# Claude Sonnet 4.5 (via Claude Code) → use as: claudecode-sonnet
# Balanced - good for complex coding and agents
LLM_CLAUDECODE_SONNET_PROVIDER=claudecode
LLM_CLAUDECODE_SONNET_MODEL=sonnet
LLM_CLAUDECODE_SONNET_URL=cli
LLM_CLAUDECODE_SONNET_KEY=

# Claude Opus 4.5 (via Claude Code) → use as: claudecode-opus
# Most capable - best for complex reasoning tasks
LLM_CLAUDECODE_OPUS_PROVIDER=claudecode
LLM_CLAUDECODE_OPUS_MODEL=opus
LLM_CLAUDECODE_OPUS_URL=cli
LLM_CLAUDECODE_OPUS_KEY=

# Benchmarking repositories
BENCH_REPO_SMALL=https://github.com/marcelemonds/flask_invoice_generator
BENCH_REPO_MEDIUM=https://github.com/fastapi/full-stack-fastapi-template
BENCH_REPO_LARGE=https://github.com/taigaio/taiga-back,https://github.com/taigaio/taiga-front
