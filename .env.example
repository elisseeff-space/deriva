# ==============================================================================
# Deriva Environment Configuration
# ==============================================================================
# Copy this file to .env and fill in your values
# .env is gitignored for security

# ==============================================================================
# NEO4J CONFIGURATION
# ==============================================================================
# Neo4j connection (shared by graph_manager and archimate_manager)
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=
NEO4J_PASSWORD=
NEO4J_DATABASE=neo4j
NEO4J_ENCRYPTED=false

# Neo4j connection pool settings
NEO4J_MAX_CONNECTION_LIFETIME=3600
NEO4J_MAX_CONNECTION_POOL_SIZE=50
NEO4J_CONNECTION_ACQUISITION_TIMEOUT=60

# Neo4j logging
NEO4J_LOG_LEVEL=INFO
NEO4J_LOG_QUERIES=false

# Manager namespaces (Neo4j label prefixes)
NEO4J_GRAPH_NAMESPACE=Graph

# ==============================================================================
# ARCHIMATE MANAGER CONFIGURATION
# ==============================================================================
# ArchiMate manager namespace (fallback: NEO4J_NAMESPACE_ARCHIMATE)
ARCHIMATE_NAMESPACE=Model
NEO4J_NAMESPACE_ARCHIMATE=Model

# Validation settings
ARCHIMATE_VALIDATION_STRICT_MODE=false

# ==============================================================================
# LLM CONFIGURATION
# ==============================================================================
# Default model: Reference a benchmark model config name (e.g., "azure-gpt4mini")
# This uses the model defined in LLM_* settings below
LLM_DEFAULT_MODEL=azure-gpt4mini

# Temperature for LLM queries (0.0-2.0, lower = more deterministic)
# Recommended: 0.3-0.5 for consistent extraction, 0.7 for general use
LLM_TEMPERATURE=0.6

# LLM cache settings
LLM_CACHE_DIR=workspace/cache
LLM_CACHE_TTL=0
LLM_MAX_RETRIES=3
LLM_TIMEOUT=60
LLM_MAX_TOKENS=

# Disable caching (true/false)
LLM_NOCACHE=false

# Rate limiting settings
# 0 = use provider-specific default (azure/openai/anthropic: 60 RPM, ollama/lmstudio: unlimited)
LLM_RATE_LIMIT_RPM=0
# Minimum delay between requests in seconds (0.0 = no minimum delay)
LLM_RATE_LIMIT_DELAY=0.0
# Max retries on rate limit (429) errors before failing
LLM_RATE_LIMIT_RETRIES=3

# ==============================================================================
# LLM PROVIDER FALLBACK CONFIGURATION
# ==============================================================================
# Used only when LLM_DEFAULT_MODEL is not set and no model is specified
LLM_PROVIDER=azure

# Azure fallback settings
LLM_AZURE_API_URL=https://your-resource.openai.azure.com/openai/deployments/your-model/chat/completions?api-version=2024-08-01-preview
LLM_AZURE_API_KEY=your-azure-api-key
LLM_AZURE_MODEL=gpt-4o-mini

# OpenAI fallback settings
LLM_OPENAI_API_KEY=your-openai-api-key
LLM_OPENAI_MODEL=gpt-4o-mini

# Anthropic fallback settings
LLM_ANTHROPIC_API_KEY=your-anthropic-api-key
LLM_ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Ollama fallback settings (local)
LLM_OLLAMA_API_URL=http://localhost:11434/api/chat
LLM_OLLAMA_MODEL=llama3.2

# LM Studio fallback settings (local)
LLM_LMSTUDIO_API_URL=http://localhost:1234/v1/chat/completions
LLM_LMSTUDIO_MODEL=local-model

# ==============================================================================
# BENCHMARK MODEL CONFIGURATIONS
# ==============================================================================
# Naming: LLM_{NAME}_* -> CLI name is lowercase with hyphens (AZURE_GPT4MINI -> azure-gpt4mini)
#
# Required fields per model:
#   LLM_{NAME}_PROVIDER  - Provider type: azure, openai, anthropic, ollama, lmstudio, mistral, claudecode
#   LLM_{NAME}_MODEL     - Model name/identifier
#   LLM_{NAME}_URL       - API endpoint URL
#   LLM_{NAME}_KEY       - API key (optional for local providers)
#   LLM_{NAME}_KEY_ENV   - Env var name containing API key (alternative to _KEY)
# ==============================================================================

# --- CLOUD PROVIDERS ---

# Mistral Devstral (cloud) -> use as: mistral-devstral
# Fast code-focused model, good for structured extraction
LLM_MISTRAL_DEVSTRAL_PROVIDER=mistral
LLM_MISTRAL_DEVSTRAL_MODEL=devstral-2512
LLM_MISTRAL_DEVSTRAL_URL=https://api.mistral.ai/v1/chat/completions
LLM_MISTRAL_DEVSTRAL_KEY=your-mistral-api-key

# Azure GPT-4o-mini (cloud) -> use as: azure-gpt4mini
# Fast cloud model, reliable structured JSON output
LLM_AZURE_GPT4MINI_PROVIDER=azure
LLM_AZURE_GPT4MINI_MODEL=gpt-4o-mini
LLM_AZURE_GPT4MINI_URL=https://your-resource.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-08-01-preview
LLM_AZURE_GPT4MINI_KEY=your-azure-api-key

# OpenAI GPT-4o-mini (cloud) -> use as: openai-gpt4omini
LLM_OPENAI_GPT4OMINI_PROVIDER=openai
LLM_OPENAI_GPT4OMINI_MODEL=gpt-4o-mini
LLM_OPENAI_GPT4OMINI_URL=https://api.openai.com/v1/chat/completions
LLM_OPENAI_GPT4OMINI_KEY=your-openai-api-key

# --- LOCAL PROVIDERS (Ollama) ---

# Ollama devstral-small (local) -> use as: ollama-devstral
# Mistral's code-focused model
LLM_OLLAMA_DEVSTRAL_PROVIDER=ollama
LLM_OLLAMA_DEVSTRAL_MODEL=devstral-small-2
LLM_OLLAMA_DEVSTRAL_URL=http://localhost:11434/api/chat

# Ollama Nemotron (local) -> use as: ollama-nemotron
# NVIDIA's reasoning model
LLM_OLLAMA_NEMOTRON_PROVIDER=ollama
LLM_OLLAMA_NEMOTRON_MODEL=nemotron-3-nano:30b
LLM_OLLAMA_NEMOTRON_URL=http://localhost:11434/api/chat

# --- LOCAL PROVIDERS (LM Studio) ---

# LM Studio local model -> use as: lmstudio-local
# Uses whatever model is loaded in LM Studio
LLM_LMSTUDIO_LOCAL_PROVIDER=lmstudio
LLM_LMSTUDIO_LOCAL_MODEL=local-model
LLM_LMSTUDIO_LOCAL_URL=http://localhost:1234/v1/chat/completions

# --- CLAUDE CODE (via Agent SDK CLI) ---

# Uses your authenticated Claude Code session - no API key needed!
# Requires: Claude Code extension installed and authenticated
#
# Model Pricing (per Million Tokens):
#   haiku:  $1/$5   (fastest, cheapest)
#   sonnet: $3/$15  (balanced)
#   opus:   $5/$25  (most capable)

# Claude Haiku (via Claude Code) -> use as: claudecode-haiku
LLM_CLAUDECODE_HAIKU_PROVIDER=claudecode
LLM_CLAUDECODE_HAIKU_MODEL=haiku
LLM_CLAUDECODE_HAIKU_URL=cli
LLM_CLAUDECODE_HAIKU_KEY=

# Claude Sonnet (via Claude Code) -> use as: claudecode-sonnet
LLM_CLAUDECODE_SONNET_PROVIDER=claudecode
LLM_CLAUDECODE_SONNET_MODEL=sonnet
LLM_CLAUDECODE_SONNET_URL=cli
LLM_CLAUDECODE_SONNET_KEY=

# Claude Opus (via Claude Code) -> use as: claudecode-opus
LLM_CLAUDECODE_OPUS_PROVIDER=claudecode
LLM_CLAUDECODE_OPUS_MODEL=opus
LLM_CLAUDECODE_OPUS_URL=cli
LLM_CLAUDECODE_OPUS_KEY=

# ==============================================================================
# REPOSITORY MANAGER CONFIGURATION
# ==============================================================================
# Workspace directory for cloned repositories and runtime data
REPOSITORY_WORKSPACE_DIR=workspace/repositories

# ==============================================================================
# DATABASE CONFIGURATION (DuckDB)
# ==============================================================================
# Note: Database path is hardcoded at deriva/adapters/database/sql.db
# DATABASE_PATH env var is not currently used by the database adapter
